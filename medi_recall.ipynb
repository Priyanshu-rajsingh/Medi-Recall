{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1zDks5noCd29",
        "outputId": "e1b2fd3b-b916-4d0a-e426-ba7ec73185c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting qdrant-client\n",
            "  Downloading qdrant_client-1.16.2-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.2.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n",
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.12/dist-packages (0.8.6)\n",
            "Requirement already satisfied: grpcio>=1.41.0 in /usr/local/lib/python3.12/dist-packages (from qdrant-client) (1.76.0)\n",
            "Requirement already satisfied: httpx>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from httpx[http2]>=0.20.0->qdrant-client) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.26 in /usr/local/lib/python3.12/dist-packages (from qdrant-client) (2.0.2)\n",
            "Collecting portalocker<4.0,>=2.7.0 (from qdrant-client)\n",
            "  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: protobuf>=3.20.0 in /usr/local/lib/python3.12/dist-packages (from qdrant-client) (5.29.5)\n",
            "Requirement already satisfied: pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8 in /usr/local/lib/python3.12/dist-packages (from qdrant-client) (2.12.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.26.14 in /usr/local/lib/python3.12/dist-packages (from qdrant-client) (2.5.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.36.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.29.0)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.187.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.43.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.27.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core->google-generativeai) (1.72.0)\n",
            "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (6.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (4.12.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (0.16.0)\n",
            "Requirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.12/dist-packages (from httpx[http2]>=0.20.0->qdrant-client) (4.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.31.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.3.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n",
            "Requirement already satisfied: hyperframe<7,>=6.1 in /usr/local/lib/python3.12/dist-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (6.1.0)\n",
            "Requirement already satisfied: hpack<5,>=4.1 in /usr/local/lib/python3.12/dist-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (4.1.0)\n",
            "Requirement already satisfied: pyparsing<4,>=3.0.4 in /usr/local/lib/python3.12/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.3.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Downloading qdrant_client-1.16.2-py3-none-any.whl (377 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m377.2/377.2 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading portalocker-3.2.0-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: portalocker, qdrant-client\n",
            "Successfully installed portalocker-3.2.0 qdrant-client-1.16.2\n"
          ]
        }
      ],
      "source": [
        "!pip install qdrant-client sentence-transformers transformers torch pillow google-generativeai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.models import Distance, VectorParams\n",
        "import time\n",
        "\n",
        "# --- FIX YOUR CREDENTIALS HERE ---\n",
        "# Make sure the URL looks like 'https://id-string.cloud.qdrant.io'\n",
        "QDRANT_URL = \"https://649869b2-dab1-4566-9ef3-158ea543cc00.europe-west3-0.gcp.cloud.qdrant.io\"\n",
        "QDRANT_API_KEY = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIn0.FH_J_qe0G5Puu0jtbovgMVRT_kYLkFZmVD-8nJgIDr4\"\n",
        "COLLECTION_NAME = \"medi_recall_memory\"\n",
        "\n",
        "# 1. Initialize Client\n",
        "try:\n",
        "    client = QdrantClient(\n",
        "        url=QDRANT_URL,\n",
        "        api_key=QDRANT_API_KEY,\n",
        "    )\n",
        "    # Test connection immediately\n",
        "    client.get_collections()\n",
        "    print(\"‚úÖ Successfully connected to Qdrant Cloud!\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Connection Failed! Check your URL and API Key.\\nError: {e}\")\n",
        "\n",
        "# 2. Create Collection (Modern Way)\n",
        "def setup_collection():\n",
        "    # Check if exists, delete if it does (to start fresh for the hackathon)\n",
        "    if client.collection_exists(COLLECTION_NAME):\n",
        "        print(f\"Collection {COLLECTION_NAME} exists. Deleting to refresh...\")\n",
        "        client.delete_collection(COLLECTION_NAME)\n",
        "\n",
        "    # Create new collection\n",
        "    client.create_collection(\n",
        "        collection_name=COLLECTION_NAME,\n",
        "        vectors_config=VectorParams(size=512, distance=Distance.COSINE),\n",
        "    )\n",
        "    print(f\"‚úÖ Collection '{COLLECTION_NAME}' is ready and fresh!\")\n",
        "\n",
        "setup_collection()"
      ],
      "metadata": {
        "id": "OuSjSlYxGRKE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cc442bd-fc97-44b6-d59a-7eed7701b838"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Successfully connected to Qdrant Cloud!\n",
            "‚úÖ Collection 'medi_recall_memory' is ready and fresh!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Multimodal model (This might take a minute to download the first time)\n",
        "print(\"Loading AI model... please wait.\")\n",
        "model = SentenceTransformer('clip-ViT-B-32')\n",
        "print(\"‚úÖ Model loaded successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TM3tDKvrOLfR",
        "outputId": "1d2a113f-3c6d-46e5-d14c-d957a922e4b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading AI model... please wait.\n",
            "‚úÖ Model loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from qdrant_client.models import PointStruct\n",
        "\n",
        "# Define your paths\n",
        "TEXT_DIR = \"/content/data/text_logs\"\n",
        "IMAGE_DIR = \"/content/data/images\"\n",
        "\n",
        "def ingest_all_data():\n",
        "    points = []\n",
        "\n",
        "    # --- 1. Process Text Files ---\n",
        "    print(\"Processing text logs...\")\n",
        "    text_files = [f for f in os.listdir(TEXT_DIR) if f.endswith('.txt')]\n",
        "    for i, filename in enumerate(text_files):\n",
        "        with open(os.path.join(TEXT_DIR, filename), 'r') as f:\n",
        "            content = f.read()\n",
        "\n",
        "        # Turn text into a vector\n",
        "        vector = model.encode(content).tolist()\n",
        "\n",
        "        points.append(PointStruct(\n",
        "            id=i,\n",
        "            vector=vector,\n",
        "            payload={\n",
        "                \"type\": \"text_log\",\n",
        "                \"filename\": filename,\n",
        "                \"content\": content,\n",
        "                \"category\": \"Medical Record\"\n",
        "            }\n",
        "        ))\n",
        "\n",
        "    # --- 2. Process Image Files ---\n",
        "    print(\"Processing images...\")\n",
        "    image_files = [f for f in os.listdir(IMAGE_DIR) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "    for i, filename in enumerate(image_files):\n",
        "        img_path = os.path.join(IMAGE_DIR, filename)\n",
        "        img = Image.open(img_path)\n",
        "\n",
        "        # Turn image into a vector\n",
        "        vector = model.encode(img).tolist()\n",
        "\n",
        "        points.append(PointStruct(\n",
        "            id=i + 1000, # Using high IDs to avoid overlap with text\n",
        "            vector=vector,\n",
        "            payload={\n",
        "                \"type\": \"image_scan\",\n",
        "                \"filename\": filename,\n",
        "                \"content\": f\"A medical image/scan named {filename}\",\n",
        "                \"category\": \"Visual Data\"\n",
        "            }\n",
        "        ))\n",
        "\n",
        "    # --- 3. Upload to Qdrant ---\n",
        "    client.upsert(collection_name=COLLECTION_NAME, points=points)\n",
        "    print(f\"‚úÖ Successfully uploaded {len(points)} items to Qdrant!\")\n",
        "\n",
        "# Run the ingestion\n",
        "ingest_all_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zt8qAZJIPUFd",
        "outputId": "d9dad627-fc14-41ca-b2bd-ff4c5f5b5d6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing text logs...\n",
            "Processing images...\n",
            "‚úÖ Successfully uploaded 10 items to Qdrant!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-initialize the connection\n",
        "client = QdrantClient(\n",
        "    url=\"https://649869b2-dab1-4566-9ef3-158ea543cc00.europe-west3-0.gcp.cloud.qdrant.io\",\n",
        "    api_key=\"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIn0.FH_J_qe0G5Puu0jtbovgMVRT_kYLkFZmVD-8nJgIDr4\"\n",
        ")\n",
        "print(\"‚úÖ Client Re-initialized!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sp6E-XFdVFhA",
        "outputId": "a0fecdd0-606a-49a5-89ae-bf978abc4ae6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Client Re-initialized!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def search_memory(query, limit=3):\n",
        "    # 1. Turn your question into a vector\n",
        "    query_vector = model.encode(query).tolist()\n",
        "\n",
        "    # 2. Search using the modern 'query_points' method\n",
        "    response = client.query_points(\n",
        "        collection_name=COLLECTION_NAME,\n",
        "        query=query_vector,\n",
        "        limit=limit\n",
        "    )\n",
        "\n",
        "    # The results are inside response.points\n",
        "    results = response.points\n",
        "\n",
        "    print(f\"\\nüîç Search Query: '{query}'\")\n",
        "    print(\"-\" * 50)\n",
        "    for res in results:\n",
        "        # Accessing payload correctly for the new method\n",
        "        p = res.payload\n",
        "        print(f\"Score: {res.score:.2f} | Source: {p.get('filename', 'Unknown')}\")\n",
        "        print(f\"Excerpt: {str(p.get('content', ''))[:150]}...\")\n",
        "        print(\"-\" * 20)\n",
        "\n",
        "    return results\n",
        "\n",
        "# --- TEST IT OUT AGAIN ---\n",
        "print(\"Testing search...\")\n",
        "try:\n",
        "    search_memory(\"What did the doctor say about blood pressure?\")\n",
        "    search_memory(\"Show me my skin rash photos\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Still getting an error: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AhrnYTfEVOW9",
        "outputId": "a86d0abd-8caf-496e-c36b-b910d38f948a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing search...\n",
            "\n",
            "üîç Search Query: 'What did the doctor say about blood pressure?'\n",
            "--------------------------------------------------\n",
            "Score: 0.81 | Source: record1_doctors_note.txt\n",
            "Excerpt: Patient: Alex\n",
            "Doctor's Note:\n",
            "Blood pressure readings are consistently high (around 148/92 mmHg).\n",
            "Patient advised to reduce salt intake, exercise regul...\n",
            "--------------------\n",
            "Score: 0.76 | Source: record4_daily_log.txt\n",
            "Excerpt: Patient: Alex\n",
            "Daily Log Entry:\n",
            "\"I felt dizzy after taking my morning pill.\" ...\n",
            "--------------------\n",
            "Score: 0.76 | Source: record2_prescription.txt\n",
            "Excerpt: Patient: Alex\n",
            "Prescription:\n",
            "Medication: Amoxicillin 500 mg\n",
            "Dosage: One capsule orally every 8 hours\n",
            "Duration: 7 days\n",
            "Instructions: Complete the full c...\n",
            "--------------------\n",
            "\n",
            "üîç Search Query: 'Show me my skin rash photos'\n",
            "--------------------------------------------------\n",
            "Score: 0.73 | Source: record5_appointment_reminder.txt\n",
            "Excerpt: Patient: Alex\n",
            "Follow-Up Appointment Reminder:\n",
            "Your follow-up appointment is scheduled for next Tuesday.\n",
            "Please arrive 15 minutes early and bring your ...\n",
            "--------------------\n",
            "Score: 0.73 | Source: record2_prescription.txt\n",
            "Excerpt: Patient: Alex\n",
            "Prescription:\n",
            "Medication: Amoxicillin 500 mg\n",
            "Dosage: One capsule orally every 8 hours\n",
            "Duration: 7 days\n",
            "Instructions: Complete the full c...\n",
            "--------------------\n",
            "Score: 0.72 | Source: record4_daily_log.txt\n",
            "Excerpt: Patient: Alex\n",
            "Daily Log Entry:\n",
            "\"I felt dizzy after taking my morning pill.\" ...\n",
            "--------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U google-generativeai"
      ],
      "metadata": {
        "id": "Q9ITmTtpVj_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google import genai\n",
        "import os\n",
        "\n",
        "# --- 1. SETUP THE NEW GEMINI CLIENT ---\n",
        "# Use the same API Key you already have\n",
        "GEMINI_API_KEY = \"AIzaSyCr5JMzAsQDhFiezqbbujfwClwl64RyfyQ\"\n",
        "\n",
        "# Initialize the new Google GenAI client\n",
        "ai_client = genai.Client(api_key=GEMINI_API_KEY)\n",
        "\n",
        "# --- 2. THE UPDATED BRAIN ---\n",
        "def ask_medi_recall(user_question):\n",
        "    # A. Search Qdrant for relevant memories (This part stays the same)\n",
        "    query_vector = model.encode(user_question).tolist()\n",
        "    response = client.query_points(\n",
        "        collection_name=COLLECTION_NAME,\n",
        "        query=query_vector,\n",
        "        limit=4\n",
        "    )\n",
        "\n",
        "    # B. Format the \"Context\"\n",
        "    context_text = \"\"\n",
        "    found_sources = []\n",
        "\n",
        "    for res in response.points:\n",
        "        p = res.payload\n",
        "        source_info = f\"Source: {p['filename']} ({p['type']})\"\n",
        "        found_sources.append(source_info)\n",
        "        context_text += f\"\\n[{source_info}]\\n{p['content']}\\n\"\n",
        "\n",
        "    # C. Create the Prompt\n",
        "    prompt = f\"\"\"\n",
        "    You are 'Medi-Recall', a personalized Healthcare AI Assistant for a patient named Alex.\n",
        "    Your goal is to help Alex remember medical details by looking at his past records and images stored in Qdrant.\n",
        "\n",
        "    RULES:\n",
        "    1. Use ONLY the 'Past Medical Memory' provided below.\n",
        "    2. If you find a relevant image (visual_scan), tell Alex the filename.\n",
        "    3. If the answer isn't in the memory, say \"I don't have a specific memory of that.\"\n",
        "    4. Be compassionate.\n",
        "\n",
        "    Past Medical Memory:\n",
        "    {context_text}\n",
        "\n",
        "    Alex's Question: {user_question}\n",
        "    \"\"\"\n",
        "\n",
        "    # D. Generate Response using the NEW library syntax\n",
        "    try:\n",
        "        ai_response = ai_client.models.generate_content(\n",
        "            model=\"gemini-1.5-flash\",\n",
        "            contents=prompt\n",
        "        )\n",
        "\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"QUESTION: {user_question}\")\n",
        "        print(f\"{'='*60}\")\n",
        "        print(f\"MEDI-RECALL: {ai_response.text}\")\n",
        "        print(f\"\\n(Evidence gathered from: {', '.join(set(found_sources))})\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå AI Error: {e}\")\n",
        "\n",
        "# --- 3. RUN THE FINAL TESTS ---\n",
        "print(\"Ready! Running tests...\")\n",
        "ask_medi_recall(\"Why have I been feeling dizzy lately?\")\n",
        "ask_medi_recall(\"What are my blood pressure trends?\")\n",
        "ask_medi_recall(\"I think I have a rash. Do we have any photos of it from before?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "Ea4cNnOMV4S2",
        "outputId": "1a387745-c46a-40e2-c7ac-74340958a964"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ready! Running tests...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-418567131.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;31m# --- 3. RUN THE FINAL TESTS ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Ready! Running tests...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m \u001b[0mask_medi_recall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Why have I been feeling dizzy lately?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0mask_medi_recall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"What are my blood pressure trends?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0mask_medi_recall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"I think I have a rash. Do we have any photos of it from before?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-418567131.py\u001b[0m in \u001b[0;36mask_medi_recall\u001b[0;34m(user_question)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mask_medi_recall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_question\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# A. Search Qdrant for relevant memories (This part stays the same)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mquery_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_question\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     response = client.query_points(\n\u001b[1;32m     16\u001b[0m         \u001b[0mcollection_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCOLLECTION_NAME\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from qdrant_client import QdrantClient\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from google import genai\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "# --- 1. RE-CONNECT EVERYTHING ---\n",
        "# (Your Qdrant details)\n",
        "QDRANT_URL = \"https://649869b2-dab1-4566-9ef3-158ea543cc00.europe-west3-0.gcp.cloud.qdrant.io\"\n",
        "QDRANT_API_KEY = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIn0.FH_J_qe0G5Puu0jtbovgMVRT_kYLkFZmVD-8nJgIDr4\"\n",
        "COLLECTION_NAME = \"medi_recall_memory\"\n",
        "\n",
        "# (Your Gemini details)\n",
        "GEMINI_API_KEY = \"AIzaSyCr5JMzAsQDhFiezqbbujfwClwl64RyfyQ\"\n",
        "\n",
        "# Initialize Clients\n",
        "client = QdrantClient(url=QDRANT_URL, api_key=QDRANT_API_KEY)\n",
        "ai_client = genai.Client(api_key=GEMINI_API_KEY)\n",
        "\n",
        "# Initialize Model (The one that turns text/images into vectors)\n",
        "print(\"Loading model... please wait a moment.\")\n",
        "model = SentenceTransformer('clip-ViT-B-32')\n",
        "print(\"‚úÖ All systems initialized and ready!\")\n",
        "\n",
        "# --- 2. THE ASSISTANT FUNCTION ---\n",
        "def ask_medi_recall(user_question):\n",
        "    try:\n",
        "        # A. Search Qdrant for relevant memories\n",
        "        query_vector = model.encode(user_question).tolist()\n",
        "        response = client.query_points(\n",
        "            collection_name=COLLECTION_NAME,\n",
        "            query=query_vector,\n",
        "            limit=4\n",
        "        )\n",
        "\n",
        "        # B. Format the \"Context\"\n",
        "        context_text = \"\"\n",
        "        found_sources = []\n",
        "\n",
        "        for res in response.points:\n",
        "            p = res.payload\n",
        "            source_info = f\"Source: {p['filename']} ({p['type']})\"\n",
        "            found_sources.append(source_info)\n",
        "            context_text += f\"\\n[{source_info}]\\n{p['content']}\\n\"\n",
        "\n",
        "        # C. Create the Prompt\n",
        "        prompt = f\"\"\"\n",
        "        You are 'Medi-Recall', a personalized Healthcare AI Assistant for Alex.\n",
        "        Your goal is to help Alex remember medical details by looking at his past records and images stored in Qdrant.\n",
        "\n",
        "        RULES:\n",
        "        1. Use ONLY the 'Past Medical Memory' provided below.\n",
        "        2. If you find a relevant image (visual_scan), tell Alex the filename.\n",
        "        3. If the answer isn't in the memory, say \"I don't have a specific memory of that.\"\n",
        "        4. Be compassionate but concise.\n",
        "\n",
        "        Past Medical Memory:\n",
        "        {context_text}\n",
        "\n",
        "        Alex's Question: {user_question}\n",
        "        \"\"\"\n",
        "\n",
        "        # D. Generate Response\n",
        "        ai_response = ai_client.models.generate_content(\n",
        "            model=\"gemini-1.5-flash\",\n",
        "            contents=prompt\n",
        "        )\n",
        "\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"QUESTION: {user_question}\")\n",
        "        print(f\"{'='*60}\")\n",
        "        print(f\"MEDI-RECALL: {ai_response.text}\")\n",
        "        print(f\"\\n(Evidence gathered from: {', '.join(set(found_sources))})\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error: {e}\")\n",
        "\n",
        "# --- 3. RUN THE FINAL TESTS ---\n",
        "print(\"\\nRunning Final Tests...\")\n",
        "ask_medi_recall(\"Why have I been feeling dizzy lately?\")\n",
        "ask_medi_recall(\"What are my blood pressure trends?\")\n",
        "ask_medi_recall(\"I think I have a rash. Do we have any photos of it from before?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJv4fGbaV_ok",
        "outputId": "a90fe1c9-d953-4760-bbc4-538346b5be2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model... please wait a moment.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ All systems initialized and ready!\n",
            "\n",
            "Running Final Tests...\n",
            "‚ùå Error: 404 NOT_FOUND. {'error': {'code': 404, 'message': 'models/gemini-1.5-flash is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.', 'status': 'NOT_FOUND'}}\n",
            "‚ùå Error: 404 NOT_FOUND. {'error': {'code': 404, 'message': 'models/gemini-1.5-flash is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.', 'status': 'NOT_FOUND'}}\n",
            "‚ùå Error: 404 NOT_FOUND. {'error': {'code': 404, 'message': 'models/gemini-1.5-flash is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.', 'status': 'NOT_FOUND'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "from qdrant_client import QdrantClient\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import os\n",
        "\n",
        "# --- 1. RE-CONNECT ---\n",
        "QDRANT_URL = \"https://649869b2-dab1-4566-9ef3-158ea543cc00.europe-west3-0.gcp.cloud.qdrant.io\"\n",
        "QDRANT_API_KEY = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIn0.FH_J_qe0G5Puu0jtbovgMVRT_kYLkFZmVD-8nJgIDr4\"\n",
        "GEMINI_API_KEY = \"AIzaSyCr5JMzAsQDhFiezqbbujfwClwl64RyfyQ\"\n",
        "COLLECTION_NAME = \"medi_recall_memory\"\n",
        "\n",
        "# Initialize Qdrant and CLIP\n",
        "client = QdrantClient(url=QDRANT_URL, api_key=QDRANT_API_KEY)\n",
        "model = SentenceTransformer('clip-ViT-B-32')\n",
        "\n",
        "# Setup Gemini\n",
        "genai.configure(api_key=GEMINI_API_KEY)\n",
        "\n",
        "# --- 2. FIND THE WORKING MODEL ---\n",
        "# This loop finds which model your API key actually has access to\n",
        "working_model = \"gemini-1.5-flash\" # Default\n",
        "try:\n",
        "    available_models = [m.name for m in genai.list_models() if 'generateContent' in m.supported_generation_methods]\n",
        "    if 'models/gemini-1.5-flash' in available_models:\n",
        "        working_model = 'gemini-1.5-flash'\n",
        "    elif 'models/gemini-pro' in available_models:\n",
        "        working_model = 'gemini-pro'\n",
        "    else:\n",
        "        # Just take the first one available\n",
        "        working_model = available_models[0].replace('models/', '')\n",
        "    print(f\"‚úÖ Using working model: {working_model}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Could not list models, defaulting to gemini-1.5-flash. Error: {e}\")\n",
        "\n",
        "# --- 3. THE UPDATED ASSISTANT ---\n",
        "def ask_medi_recall(user_question):\n",
        "    try:\n",
        "        # A. Search Qdrant\n",
        "        query_vector = model.encode(user_question).tolist()\n",
        "        response = client.query_points(\n",
        "            collection_name=COLLECTION_NAME,\n",
        "            query=query_vector,\n",
        "            limit=4\n",
        "        )\n",
        "\n",
        "        context_text = \"\"\n",
        "        found_sources = []\n",
        "        for res in response.points:\n",
        "            p = res.payload\n",
        "            source_info = f\"Source: {p['filename']} ({p['type']})\"\n",
        "            found_sources.append(source_info)\n",
        "            context_text += f\"\\n[{source_info}]\\n{p['content']}\\n\"\n",
        "\n",
        "        # B. Generate response using the model we found\n",
        "        prompt = f\"\"\"\n",
        "        You are 'Medi-Recall', Alex's Healthcare AI.\n",
        "        Use the following medical memory to answer his question.\n",
        "        Memory: {context_text}\n",
        "        Question: {user_question}\n",
        "        \"\"\"\n",
        "\n",
        "        # Create the model object\n",
        "        llm = genai.GenerativeModel(working_model)\n",
        "        ai_response = llm.generate_content(prompt)\n",
        "\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"MEDI-RECALL: {ai_response.text}\")\n",
        "        print(f\"{'='*60}\")\n",
        "        print(f\"Evidence: {', '.join(set(found_sources))}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error: {e}\")\n",
        "\n",
        "# --- 4. FINAL RUN ---\n",
        "print(\"System Ready. Testing...\")\n",
        "ask_medi_recall(\"Why have I been feeling dizzy lately?\")\n",
        "ask_medi_recall(\"Show me my skin rash photos\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 627
        },
        "id": "TuNgJym2YYRt",
        "outputId": "ec2b5e98-5ba0-40ef-ed32-9e2c80257469"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/google/colab/_import_hooks/_hook_injector.py:55: FutureWarning: \n",
            "\n",
            "All support for the `google.generativeai` package has ended. It will no longer be receiving \n",
            "updates or bug fixes. Please switch to the `google.genai` package as soon as possible.\n",
            "See README for more details:\n",
            "\n",
            "https://github.com/google-gemini/deprecated-generative-ai-python/blob/main/README.md\n",
            "\n",
            "  loader.exec_module(module)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Using working model: gemini-2.5-flash\n",
            "System Ready. Testing...\n",
            "\n",
            "============================================================\n",
            "MEDI-RECALL: Alex, based on your medical memory:\n",
            "\n",
            "You have reported feeling dizzy, specifically after taking your morning pill.\n",
            "\n",
            "A significant factor noted in your doctor's record is that your **blood pressure readings are consistently high (around 148/92 mmHg)**. High blood pressure can sometimes cause symptoms like dizziness.\n",
            "\n",
            "While you are currently taking Amoxicillin, dizziness is not a common side effect of this medication. However, it's important to consider that your underlying high blood pressure, or potentially any medication you are taking (including your \"morning pill\" if it's not Amoxicillin, or even Amoxicillin in your specific case), could be contributing to these feelings.\n",
            "\n",
            "It's crucial to discuss this with your doctor. Please remember your follow-up appointment is scheduled for next Tuesday, and it would be beneficial to bring your current medication list as requested, so your doctor can evaluate your symptoms in light of your blood pressure and medications.\n",
            "============================================================\n",
            "Evidence: Source: record5_appointment_reminder.txt (text_log), Source: record1_doctors_note.txt (text_log), Source: record2_prescription.txt (text_log), Source: record4_daily_log.txt (text_log)\n",
            "\n",
            "============================================================\n",
            "MEDI-RECALL: I'm sorry, Alex. I don't have any records of your skin rash photos in my current memory. My available records for you include:\n",
            "\n",
            "*   **Follow-Up Appointment Reminder:** Scheduled for next Tuesday. Please arrive 15 minutes early with your current medication list.\n",
            "*   **Prescription:** Amoxicillin 500 mg, one capsule orally every 8 hours for 7 days. Remember to complete the full course.\n",
            "*   **Daily Log Entry:** You felt dizzy after taking your morning pill.\n",
            "*   **Lab Report:** Indicates Vitamin D deficiency, with a recommendation for supplementation and increased sunlight exposure.\n",
            "============================================================\n",
            "Evidence: Source: record2_prescription.txt (text_log), Source: record3_lab_report.txt (text_log), Source: record5_appointment_reminder.txt (text_log), Source: record4_daily_log.txt (text_log)\n"
          ]
        }
      ]
    }
  ]
}